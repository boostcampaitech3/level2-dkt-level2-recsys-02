{"cells":[{"cell_type":"markdown","metadata":{"id":"yt7sDAqHhfQp"},"source":["# LGBM을 활용한 베이스라인"]},{"cell_type":"code","execution_count":1,"metadata":{"ExecuteTime":{"end_time":"2021-05-24T09:49:29.375544Z","start_time":"2021-05-24T09:49:28.999092Z"},"id":"Uq_TJqbdhfQu"},"outputs":[],"source":["import pandas as pd\n","import os\n","import random\n","from tqdm import tqdm\n","import numpy as np\n","import lightgbm as lgb\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set_theme(color_codes=True)\n","import pickle\n","import optuna"]},{"cell_type":"markdown","metadata":{"id":"QZlm5HSmhfQv"},"source":["## 1. 데이터 로딩"]},{"cell_type":"code","execution_count":2,"metadata":{"ExecuteTime":{"end_time":"2021-05-24T09:49:29.678737Z","start_time":"2021-05-24T09:49:29.376581Z"},"id":"s6qgJ8MLhfQw"},"outputs":[],"source":["data_dir = '/opt/ml/project/data/'\n","csv_file_path = os.path.join(data_dir, 'total_data_v2.csv')\n","df = pd.read_csv(csv_file_path, parse_dates=['Timestamp']) \n","df = df.sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{"id":"p_oCGAgEhfQw"},"source":["## 2. Feature Engineering"]},{"cell_type":"code","execution_count":3,"metadata":{"ExecuteTime":{"end_time":"2021-05-24T09:49:29.682739Z","start_time":"2021-05-24T09:49:28.979Z"},"id":"URNLoukChfQx"},"outputs":[],"source":["def load_agg_data(df):\n","    last_cond = df['userID'] != df['userID'].shift(-1)\n","    agg_df = df[~last_cond]\n","    return agg_df\n","\n","\n","def calculate_elapsdTime(df):\n","    cond = df['elapsedTime'].isna()\n","    global_elapsedTime_mean = df['elapsedTime'].mean()\n","    df['estimated_elapsedTime'] = cond\n","    \n","    for window_size in [3, 5, 10, 30, 50, 100, 200]:\n","        df[f'roll_elapsedTime_mean{window_size}'] = \\\n","            df.groupby(['userID'])[f'elapsedTime'].rolling(window_size, min_periods=1).mean().values\n","    \n","    # version 1\n","    df[f'elapsedTime_v1'] = df['elapsedTime'].values\n","    df.loc[cond, f'elapsedTime_v1'] = global_elapsedTime_mean\n","    \n","    # version 2\n","    df[f'elapsedTime_v2'] = df['elapsedTime'].values\n","    question_time_dict = df.groupby('assessmentItemID').elapsedTime.mean().to_dict()\n","    df.loc[cond, f'elapsedTime_v2'] = df.loc[cond, 'assessmentItemID'].apply(lambda x:question_time_dict[x] if x in question_time_dict.keys() else global_elapsedTime_mean)\n","    \n","    # version 3\n","    df[f'elapsedTime_v3'] = df['elapsedTime'].values\n","    user_time_dict = df.groupby('userID').elapsedTime.mean().to_dict()\n","    df.loc[cond, f'elapsedTime_v3'] = df.loc[cond, 'userID'].apply(lambda x:user_time_dict[x] if x in user_time_dict.keys() else global_elapsedTime_mean)\n","    \n","    # version 4        \n","    df[f'elapsedTime_v4'] = df['elapsedTime'].values\n","    df.loc[cond, f'elapsedTime_v4'] = df.loc[cond, f'roll_elapsedTime_mean3']\n","        \n","    return df\n","\n","\n","def calculate_user_question_elapsedTime(df):\n","    agg_df = load_agg_data(df)\n","    \n","    # 유저 별 all/correct/wrong 걸린 시간 평균\n","    user_df = agg_df.groupby('userID')[f'elapsedTime_v3'].agg(['mean'])\n","    user_correct_df = agg_df[agg_df['answerCode'] == 1].groupby('userID')[f'elapsedTime_v3'].agg(['mean'])\n","    user_wrong_df = agg_df[agg_df['answerCode'] == 0].groupby('userID')[f'elapsedTime_v3'].agg(['mean'])\n","    \n","    user_df.columns = [f'user_elapsedTime_mean']\n","    user_correct_df.columns = [f'user_correct_elapsedTime_mean']\n","    user_wrong_df.columns = [f'user_wrong_elapsedTime_mean']\n","    \n","    # 문제 별 all/correct/wrong 걸린 시간 평균\n","    question_df = agg_df.groupby('assessmentItemID')[f'elapsedTime_v2'].agg(['mean'])\n","    question_correct_df = agg_df[agg_df['answerCode'] == 1].groupby('assessmentItemID')[f'elapsedTime_v2'].agg(['mean'])\n","    question_wrong_df = agg_df[agg_df['answerCode'] == 0].groupby('assessmentItemID')[f'elapsedTime_v2'].agg(['mean'])\n","    \n","    question_df.columns = [f'question_elapsedTime_mean']\n","    question_correct_df.columns = [f'question_correct_elapsedTime_mean']\n","    question_wrong_df.columns = [f'question_wrong_elapsedTime_mean']\n","    \n","    df = pd.merge(df, user_df, on=['userID'], how=\"left\")\n","    df = pd.merge(df, user_correct_df, on=['userID'], how=\"left\")\n","    df = pd.merge(df, user_wrong_df, on=['userID'], how=\"left\")\n","    df = pd.merge(df, question_df, on=['assessmentItemID'], how=\"left\")\n","    df = pd.merge(df, question_correct_df, on=['assessmentItemID'], how=\"left\")\n","    df = pd.merge(df, question_wrong_df, on=['assessmentItemID'], how=\"left\")\n","    \n","    time_df = df.groupby('userID').elapsedTime.rolling(window=3).sum()\n","    time_df = time_df.reset_index()[['userID', 'elapsedTime']]\n","    cond1 = time_df.elapsedTime >= 0\n","    cond2 = time_df.elapsedTime < 3\n","    df['randomly_marked'] = cond1 & cond2\n","\n","    return df\n","\n","\n","def calculate_statistics(df):    \n","    agg_df = load_agg_data(df)\n","    \n","    correct_i = agg_df.groupby(['assessmentItemID'])['answerCode'].agg(['mean', 'sum', 'std'])\n","    correct_i.columns = [\"question_mean\", 'question_sum', 'question_std']\n","    correct_t = agg_df.groupby(['testId'])['answerCode'].agg(['mean', 'sum', 'std'])\n","    correct_t.columns = [\"test_mean\", 'test_sum', 'test_std']\n","    correct_k = agg_df.groupby(['KnowledgeTag'])['answerCode'].agg(['mean', 'sum', 'std'])\n","    correct_k.columns = [\"tag_mean\", 'tag_sum', 'tag_std']\n","    type_df = agg_df.groupby('testType')['answerCode'].agg(['mean', 'sum', 'std'])\n","    type_df.columns = ['type_mean', 'type_sum', 'type_std']\n","    qn_df = agg_df.groupby('questionNumber')['answerCode'].agg(['mean','sum','std'])\n","    qn_df.columns = ['question_number_mean', 'question_number_sum', 'question_number_std']\n","    tn_df = agg_df.groupby('testNumber').answerCode.agg(['mean', 'sum', 'std'])\n","    tn_df.columns = ['test_number_mean', 'test_number_sum', 'test_number_std']\n","    \n","    corr_df = agg_df.groupby('assessmentItemID')['answerCode'].agg([['corr_ratio', 'mean']]).reset_index()\n","    corr_df = agg_df[agg_df['answerCode']==0].merge(corr_df, on='assessmentItemID')\n","    corr_df = corr_df.groupby('userID')['corr_ratio'].agg(['min', 'max', 'mean', 'std']).reset_index()\n","    corr_df.columns = ['userID', 'corr_min', 'corr_max', 'corr_mean', 'corr_std']\n","    \n","    df = pd.merge(df, correct_i, on=['assessmentItemID'], how=\"left\")\n","    df = pd.merge(df, correct_t, on=['testId'], how=\"left\")\n","    df = pd.merge(df, correct_k, on=['KnowledgeTag'], how=\"left\")\n","    df = pd.merge(df, type_df, on=['testType'], how=\"left\")\n","    df = pd.merge(df, qn_df, on=['questionNumber'], how='left')\n","    df = pd.merge(df, tn_df, on=['testNumber'], how='left')\n","    df = pd.merge(df, corr_df, on=['userID'], how='left')\n","    \n","    return df\n","\n","\n","def calculate_user_accuracy(df):\n","    df['user_correct_answer'] = df.groupby('userID')['answerCode'].transform(lambda x: x.cumsum().shift(1))\n","    df['user_total_answer'] = df.groupby('userID')['answerCode'].cumcount()\n","    df['user_acc'] = df['user_correct_answer']/df['user_total_answer']\n","    return df\n","\n","\n","def calculate_accuracy_trend(df):\n","    for window_size in [3, 5, 10, 30, 50, 100, 200]:\n","        user_df = df.groupby(df['userID']).shift(1)\n","        accuracy_trend = user_df.groupby(df['userID']).answerCode.rolling(window=window_size, min_periods=1).mean()\n","        correct_trend = user_df.groupby(df['userID']).answerCode.rolling(window=window_size, min_periods=1).sum()\n","        \n","        df[f'accuracy_trend{window_size}'] = accuracy_trend.values\n","        df[f'normalized_accuracy_trend{window_size}'] = df[f'accuracy_trend{window_size}'] - df['question_mean']\n","        df[f'correct_trend{window_size}'] = correct_trend.values\n","    \n","    df['shift'] = df.groupby('userID').answerCode.shift(1)\n","    \n","    for window_size in ['10min','1h','10h','1D','10D']:\n","        temp_accuracy_arr = np.zeros(len(df))\n","        temp_correct_arr = np.zeros(len(df))\n","\n","        for user_id, temp_df in df.groupby('userID'):\n","            idx = temp_df.index\n","            accuracy_time_trend = temp_df.set_index('Timestamp')['shift'].rolling(window_size, min_periods=1).mean()\n","            correct_time_trend = temp_df.set_index('Timestamp')['shift'].rolling(window_size, min_periods=1).sum()\n","            temp_accuracy_arr[idx] = accuracy_time_trend\n","            temp_correct_arr[idx] = correct_time_trend\n","\n","        df[f'accuracy_time_trend{window_size}'] = temp_accuracy_arr\n","        df[f'normalized_accuracy_time_trend{window_size}'] = temp_accuracy_arr - df['question_mean'].values\n","        df[f'correct_time_trend{window_size}'] = temp_correct_arr\n","    \n","    return df\n","\n","\n","def calculate_accuracy_on_past_attempts(df):\n","    # 과거 똑같은 문제 count/correct/accuracy\n","    df['past_question_count'] = df.groupby(['userID', 'assessmentItemID']).cumcount()\n","    df['shift'] = df.groupby(['userID', 'assessmentItemID'])['answerCode'].shift().fillna(0)\n","    df['past_question_correct'] = df.groupby(['userID', 'assessmentItemID'])['shift'].cumsum()\n","    df['past_question_accuracy'] = (df['past_question_correct'] / df['past_question_count']).fillna(0)\n","    \n","    # 과거 똑같은 태그 count/correct/accuracy\n","    df['past_tag_count'] = df.groupby(['userID', 'KnowledgeTag']).cumcount()\n","    df['shift'] = df.groupby(['userID', 'KnowledgeTag'])['answerCode'].shift().fillna(0)\n","    df['past_tag_correct'] = df.groupby(['userID', 'KnowledgeTag'])['shift'].cumsum()\n","    df['past_tag_accuracy'] = (df['past_tag_correct'] / df['past_tag_count']).fillna(0)\n","    \n","    # 과거 똑같은 Type count/correct/accuracy\n","    df['past_type_count'] = df.groupby(['userID', 'testType']).cumcount()\n","    df['shift'] = df.groupby(['userID', 'testType'])['answerCode'].shift().fillna(0)\n","    df['past_type_correct'] = df.groupby(['userID', 'testType'])['shift'].cumsum()\n","    df['past_type_accuracy'] = (df['past_type_correct'] / df['past_type_count']).fillna(0)\n","    \n","    # 과거 똑같은 문제 푼지 얼마나 되었는지\n","    shift = df.groupby(['userID', 'assessmentItemID'])['Timestamp'].shift()\n","    last_question_elapsedTime = (df['Timestamp'].values - shift.values) / np.timedelta64(1, 's')\n","    last_question_elapsedTime[np.isnan(last_question_elapsedTime)] = 0\n","    df['past_question_elapsedTime'] = last_question_elapsedTime\n","\n","    # 과거 똑같은 태그 푼지 얼마나 되었는지\n","    shift = df.groupby(['userID', 'KnowledgeTag'])['Timestamp'].shift()\n","    last_tag_elapsedTime = (df['Timestamp'].values - shift.values) / np.timedelta64(1, 's')\n","    last_tag_elapsedTime[np.isnan(last_tag_elapsedTime)] = 0\n","    df['past_tag_elapsedTime'] = last_tag_elapsedTime\n","\n","    # 과거 똑같은 Type 푼지 얼마나 되었는지\n","    shift = df.groupby(['userID', 'testType'])['Timestamp'].shift()\n","    last_tag_elapsedTime = (df['Timestamp'].values - shift.values) / np.timedelta64(1, 's')\n","    last_tag_elapsedTime[np.isnan(last_tag_elapsedTime)] = 0\n","    df['past_type_elapsedTime'] = last_tag_elapsedTime\n","    \n","    return df\n","\n","\n","def calculate_time_slot(df):\n","    # 날짜, timestamp, 시간\n","    df['day'] = df.Timestamp.dt.day\n","    df['time'] = df.Timestamp.apply(lambda x: x.value // 10**9)\n","    df['hour'] = df['Timestamp'].transform(lambda x: x.dt.hour)\n","    \n","    agg_df = load_agg_data(df)\n","    \n","    # 시간대별 정확도, 유저별 공부 시간, 야행성 여부\n","    hour_dict = agg_df.groupby(['hour'])['answerCode'].mean().to_dict()\n","    mode_dict = df.groupby(['userID'])['hour'].agg(lambda x: pd.Series.mode(x)[0]).to_dict()\n","    df['accuracy_per_hour'] = df['hour'].map(hour_dict)\n","    df['hour_mode'] = df['userID'].map(mode_dict)\n","    df['is_night'] = (df['hour_mode'] >= 22).values | (df['hour_mode'] < 4).values\n","    \n","    # Test 치는데 걸리는 시간\n","    # df['test_total_time'] = (df['test_end_time'].values - df['test_start_time'].values) / np.timedelta64(1, 's')\n","    \n","    return df\n","\n","\n","def dimension_reduction(df):\n","    # Truncated SVD\n","    SVD_DIM = 5\n","    with open('./assets/svd_question.pickle','rb') as f:\n","         svd_q_dict = pickle.load(f)\n","         \n","    svd_q_df = pd.DataFrame.from_dict(svd_q_dict).T\n","    cols = [f'svd_question{i+1}' for i in range(SVD_DIM)]\n","    cols.insert(0, 'assessmentItemID')\n","    svd_q_df = svd_q_df.reset_index()\n","    svd_q_df.columns = cols\n","    df = pd.merge(df, svd_q_df, how='left', on='assessmentItemID')\n","\n","    with open('./assets/svd_user.pickle','rb') as f:\n","         svd_u_dict = pickle.load(f)\n","         \n","    svd_u_df = pd.DataFrame.from_dict(svd_u_dict).T\n","    cols = [f'svd_user{i+1}' for i in range(SVD_DIM)]\n","    cols.insert(0, 'userID')\n","    svd_u_df = svd_u_df.reset_index()\n","    svd_u_df.columns = cols\n","    df = pd.merge(df, svd_u_df, how='left', on='userID')\n","    \n","    LDA_DIM = 5\n","    # 문제들에 대한 유저별 정답 횟수 행렬 LDA\n","    transformed = np.load(f'./assets/lda_correct_question.npy')\n","    transformed_df = pd.DataFrame(transformed)\n","    transformed_df.columns = [f'lda_correct_question{i+1}' for i in range(LDA_DIM)]\n","    transformed_df = transformed_df.astype(np.float32)\n","    transformed_df['assessmentItemID'] = sorted(df.assessmentItemID.unique())\n","    df = pd.merge(df, transformed_df, how='left', on='assessmentItemID')\n","    \n","    # 문제들에 대한 유저별 오답 횟수 행렬 LDA (answerCode == 0 filtering)\n","    wrong_transformed = np.load(f'./assets/lda_wrong_question.npy')\n","    wrong_transformed_df = pd.DataFrame(wrong_transformed)\n","    wrong_transformed_df.columns = [f'lda_wrong_question{i+1}' for i in range(LDA_DIM)]\n","    wrong_transformed_df = wrong_transformed_df.astype(np.float32)\n","    wrong_transformed_df['assessmentItemID'] = sorted(df.assessmentItemID.unique())\n","    df = pd.merge(df, wrong_transformed_df, how='left', on='assessmentItemID')\n","    \n","    return df\n","\n","\n","def word2vec_embedding(df):\n","    EMB_DIM = 10\n","    # user's correct question list word2vec \n","    with open('./assets/word2vec_correct_question.pickle','rb') as f:\n","         word2vec_correct = pickle.load(f)\n","    \n","    emb_correct_df = pd.DataFrame.from_dict(word2vec_correct).T\n","    cols = [f'word2vec_correct_question{i+1}' for i in range(EMB_DIM)]\n","    cols.insert(0, 'assessmentItemID')\n","    emb_correct_df = emb_correct_df.reset_index()\n","    emb_correct_df.columns = cols\n","    \n","    # user's wrong question list word2vec\n","    with open('./assets/word2vec_wrong_question.pickle','rb') as f:\n","         word2vec_wrong = pickle.load(f)\n","    \n","    emb_wrong_df = pd.DataFrame.from_dict(word2vec_wrong).T\n","    cols = [f'word2vec_wrong_question{i+1}' for i in range(EMB_DIM)]\n","    cols.insert(0, 'assessmentItemID')\n","    emb_wrong_df = emb_wrong_df.reset_index()\n","    emb_wrong_df.columns = cols\n","    \n","    df = pd.merge(df, emb_correct_df, how='left', on='assessmentItemID')\n","    df = pd.merge(df, emb_wrong_df, how='left', on='assessmentItemID')\n","    \n","    return df\n","\n","\n","def gcn_embedding(df):\n","    GCN_EMB_DIM = 10\n","    with open('./assets/gcn_embedding.pickle','rb') as f:\n","        gcn_embedding = pickle.load(f)\n","\n","    gcn_user_embedding = pd.DataFrame.from_dict(gcn_embedding['user']).T\n","    cols = [f'gcn_user_embedding{i+1}' for i in range(GCN_EMB_DIM)]\n","    cols.insert(0, 'userID')\n","    gcn_user_embedding = gcn_user_embedding.reset_index()\n","    gcn_user_embedding.columns = cols\n","\n","    gcn_item_embedding = pd.DataFrame.from_dict(gcn_embedding['item']).T\n","    cols = [f'gcn_question_embedding{i+1}' for i in range(GCN_EMB_DIM)]\n","    cols.insert(0, 'assessmentItemID')\n","    gcn_item_embedding = gcn_item_embedding.reset_index()\n","    gcn_item_embedding.columns = cols\n","    \n","    df = pd.merge(df, gcn_user_embedding, how='left', on='userID')\n","    df = pd.merge(df, gcn_item_embedding, how='left', on='assessmentItemID')\n","    \n","    return df\n","\n","\n","def elo_rating(df):\n","    with open('./assets/elo_student_parameters.pickle','rb') as f:\n","        student_parameters = pickle.load(f)\n","\n","    with open('./assets/elo_item_parameters.pickle','rb') as f:\n","        item_parameters = pickle.load(f)\n","\n","    student_df = pd.DataFrame.from_dict(student_parameters).T\n","    student_df.columns = ['elo_theta', 'user_nb_answers']\n","    student_df['userID'] = student_df.index\n","    item_df = pd.DataFrame.from_dict(item_parameters).T\n","    item_df.columns = ['elo_beta', 'item_nb_answers']\n","    item_df['assessmentItemID'] = item_df.index\n","\n","    df = pd.merge(df, student_df, how='left', on='userID')\n","    df = pd.merge(df, item_df, how='left', on='assessmentItemID')\n","\n","    return df\n","    \n","\n","def feature_engineering(df):\n","    \n","    df = calculate_elapsdTime(df)\n","    df = calculate_user_question_elapsedTime(df)\n","    \n","    df = calculate_statistics(df)\n","    df = calculate_user_accuracy(df)\n","    df = calculate_accuracy_trend(df)\n","    df = calculate_accuracy_on_past_attempts(df)\n","    \n","    df = calculate_time_slot(df)\n","    \n","    df = dimension_reduction(df)\n","    df = word2vec_embedding(df)\n","    # df = gcn_embedding(df)\n","    df = elo_rating(df)\n","    \n","    df.fillna(0, inplace=True)\n","    \n","    return df"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["fe_df = feature_engineering(df)"]},{"cell_type":"markdown","metadata":{"id":"5VZzei3DhfQy"},"source":["## 3. Train/Valid 데이터 셋 분리"]},{"cell_type":"code","execution_count":5,"metadata":{"ExecuteTime":{"end_time":"2021-05-24T09:49:29.684739Z","start_time":"2021-05-24T09:49:28.982Z"},"id":"YOPWK7ckhfQz"},"outputs":[],"source":["# train과 test 데이터셋은 사용자 별로 묶어서 분리를 해주어야함\n","def custom_train_test_split(df, ratio=0.7, user_ids=[]):\n","    if len(user_ids) == 0 :\n","        users = list(zip(df['userID'].value_counts().index, df['userID'].value_counts()))\n","        \n","        random.seed(42)\n","        random.shuffle(users)\n","        \n","        max_train_data_len = ratio*len(df)\n","        sum_of_train_data = 0\n","        user_ids =[]\n","\n","        for user_id, count in users:\n","            sum_of_train_data += count\n","            if max_train_data_len < sum_of_train_data:\n","                break\n","            user_ids.append(user_id)\n","\n","    train = df[df['userID'].isin(user_ids)]\n","    valid = df[df['userID'].isin(user_ids) == False]\n","\n","    #test데이터셋은 각 유저의 마지막 interaction만 추출\n","    valid = valid[valid['userID'] != valid['userID'].shift(-1)]\n","    return train, valid"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# DROP ANSWERCODE\n","test_df = fe_df[fe_df['answerCode'] == -1]\n","test_users = test_df.userID.unique()\n","test_df = test_df.drop(['answerCode'], axis=1)\n","\n","train_df = fe_df[fe_df['userID'].isin(test_users) == False]"]},{"cell_type":"code","execution_count":59,"metadata":{"ExecuteTime":{"end_time":"2021-05-24T09:49:29.686739Z","start_time":"2021-05-24T09:49:28.984Z"},"id":"i3HzdoybhfQ0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of Features: 88\n"]}],"source":["# 사용할 Feature 설정\n","FEATS = ['KnowledgeTag', \n","         \n","         'time',\n","         'hour',\n","         'hour_mode',\n","         'accuracy_per_hour',  \n","        \n","         'user_correct_answer', \n","         'user_total_answer', \n","         'user_acc',\n","         \n","         'test_mean', \n","         'test_sum', \n","         'test_std',\n","         \n","         'tag_mean',\n","         'tag_sum', \n","         'tag_std', \n","         \n","         'type_mean', \n","         'type_sum',\n","         'type_std',\n","         \n","         'question_mean',\n","         'question_sum',\n","         'question_std', \n","         \n","         'question_number_mean', \n","         'question_number_sum', \n","         'question_number_std',\n","         \n","         'test_number_mean', \n","         'test_number_sum', \n","         'test_number_std',\n","         \n","        #  'corr_min', \n","        #  'corr_max', \n","        #  'corr_mean', \n","        #  'corr_std',\n","        \n","         'accuracy_trend3', \n","         'accuracy_trend5', \n","         'accuracy_trend10', \n","         'accuracy_trend30', \n","         'accuracy_trend50', \n","         'accuracy_trend100', \n","         'accuracy_trend200',\n","        #  'accuracy_time_trend1h',\n","        #  'accuracy_time_trend10h',\n","        #  'accuracy_time_trend1D',\n","        #  'accuracy_time_trend10min',\n","        #  'accuracy_time_trend10D',\n","         \n","         'normalized_accuracy_trend3', \n","         'normalized_accuracy_trend5', \n","         'normalized_accuracy_trend10', \n","         'normalized_accuracy_trend30', \n","         'normalized_accuracy_trend50', \n","         'normalized_accuracy_trend100', \n","         'normalized_accuracy_trend200',\n","        #  'normalized_accuracy_time_trend1h',\n","        #  'normalized_accuracy_time_trend10h',\n","        #  'normalized_accuracy_time_trend1D',\n","        #  'normalized_accuracy_time_trend10min',\n","        #  'normalized_accuracy_time_trend10D',\n","         \n","         'correct_trend3', \n","         'correct_trend5', \n","         'correct_trend10', \n","         'correct_trend30', \n","         'correct_trend50', \n","         'correct_trend100', \n","         'correct_trend200',\n","        #  'correct_time_trend1h',\n","        #  'correct_time_trend10h',\n","        #  'correct_time_trend1D',\n","        #  'correct_time_trend10min',\n","        #  'correct_time_trend10D',\n","         \n","        #  'estimated_elapsedTime',\n","        #  'elapsedTime_v1', \n","        #  'elapsedTime_v2', \n","        #  'elapsedTime_v3', \n","         'elapsedTime_v4', \n","         \n","         'user_elapsedTime_mean',\n","         'user_correct_elapsedTime_mean',\n","         'user_wrong_elapsedTime_mean',\n","         'question_elapsedTime_mean',\n","         'question_correct_elapsedTime_mean',\n","         'question_wrong_elapsedTime_mean',\n","         \n","         'past_type_count',\n","         'past_type_correct',\n","         'past_type_accuracy',\n","         'past_type_elapsedTime',\n","         'past_tag_count',\n","         'past_tag_correct',\n","         'past_tag_accuracy',\n","         'past_tag_elapsedTime',\n","                 \n","         'roll_elapsedTime_mean3',\n","         'roll_elapsedTime_mean5',\n","         'roll_elapsedTime_mean10',\n","         'roll_elapsedTime_mean30',\n","         'roll_elapsedTime_mean50',\n","         'roll_elapsedTime_mean100',\n","         'roll_elapsedTime_mean200',\n","        \n","         'word2vec_wrong_question1',\n","         'word2vec_wrong_question2',\n","         'word2vec_wrong_question3',\n","         'word2vec_wrong_question4',\n","         'word2vec_wrong_question5',\n","         'word2vec_wrong_question6',\n","         'word2vec_wrong_question7',\n","         'word2vec_wrong_question8',\n","         'word2vec_wrong_question9',\n","         'word2vec_wrong_question10',\n","         \n","        #  'lda_wrong_question1',\n","        #  'lda_wrong_question2',\n","        #  'lda_wrong_question3',\n","        #  'lda_wrong_question4',\n","        #  'lda_wrong_question5',\n","        \n","        #  'svd_user1',\n","        #  'svd_user2',\n","        #  'svd_user3',\n","        #  'svd_user4',\n","        #  'svd_user5',\n","         \n","         'svd_question1',\n","         'svd_question2',\n","         'svd_question3',\n","         'svd_question4',\n","         'svd_question5',\n","\n","        #  'elo_theta',\n","        #  'elo_beta',\n","         \n","         'trueSkill_win_probability',\n","        #  'trueSkill_user_mu',\n","        #  'trueSkill_user_sigma',\n","        #  'trueSkill_question_mu',\n","        #  'trueSkill_question_sigma',\n","]\n","print(f'Number of Features: {len(FEATS)}')"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 1039565, number of negative: 546592\n","[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.159244 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 17387\n","[LightGBM] [Info] Number of data points in the train set: 1586157, number of used features: 88\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655399 -> initscore=0.642855\n","[LightGBM] [Info] Start training from score 0.642855\n","Training until validation scores don't improve for 100 rounds\n","[100]\ttraining's binary_logloss: 0.432909\tvalid_1's binary_logloss: 0.502391\n","[200]\ttraining's binary_logloss: 0.427852\tvalid_1's binary_logloss: 0.498853\n","[300]\ttraining's binary_logloss: 0.424837\tvalid_1's binary_logloss: 0.498328\n","[400]\ttraining's binary_logloss: 0.42207\tvalid_1's binary_logloss: 0.497452\n","[500]\ttraining's binary_logloss: 0.419572\tvalid_1's binary_logloss: 0.496351\n","[600]\ttraining's binary_logloss: 0.417248\tvalid_1's binary_logloss: 0.495285\n","[700]\ttraining's binary_logloss: 0.414932\tvalid_1's binary_logloss: 0.494599\n","Early stopping, best iteration is:\n","[687]\ttraining's binary_logloss: 0.415234\tvalid_1's binary_logloss: 0.494453\n","VALID1 AUC : 0.8388312462731068 ACC : 0.7673143996013951\n","\n","[LightGBM] [Info] Number of positive: 1040633, number of negative: 545631\n","[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151679 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 17391\n","[LightGBM] [Info] Number of data points in the train set: 1586264, number of used features: 88\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656028 -> initscore=0.645642\n","[LightGBM] [Info] Start training from score 0.645642\n","Training until validation scores don't improve for 100 rounds\n","[100]\ttraining's binary_logloss: 0.433924\tvalid_1's binary_logloss: 0.491844\n","[200]\ttraining's binary_logloss: 0.428853\tvalid_1's binary_logloss: 0.488125\n","[300]\ttraining's binary_logloss: 0.42576\tvalid_1's binary_logloss: 0.486467\n","[400]\ttraining's binary_logloss: 0.423067\tvalid_1's binary_logloss: 0.485215\n","[500]\ttraining's binary_logloss: 0.420608\tvalid_1's binary_logloss: 0.484934\n","[600]\ttraining's binary_logloss: 0.418196\tvalid_1's binary_logloss: 0.484316\n","[700]\ttraining's binary_logloss: 0.415948\tvalid_1's binary_logloss: 0.484481\n","[800]\ttraining's binary_logloss: 0.413825\tvalid_1's binary_logloss: 0.483789\n","[900]\ttraining's binary_logloss: 0.411707\tvalid_1's binary_logloss: 0.483681\n","[1000]\ttraining's binary_logloss: 0.409591\tvalid_1's binary_logloss: 0.483871\n","Early stopping, best iteration is:\n","[913]\ttraining's binary_logloss: 0.411415\tvalid_1's binary_logloss: 0.483515\n","VALID2 AUC : 0.8469078999322518 ACC : 0.7678207739307535\n","\n","[LightGBM] [Info] Number of positive: 1035342, number of negative: 550858\n","[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.144339 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 17397\n","[LightGBM] [Info] Number of data points in the train set: 1586200, number of used features: 88\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652718 -> initscore=0.631010\n","[LightGBM] [Info] Start training from score 0.631010\n","Training until validation scores don't improve for 100 rounds\n","[100]\ttraining's binary_logloss: 0.433713\tvalid_1's binary_logloss: 0.490033\n","[200]\ttraining's binary_logloss: 0.428614\tvalid_1's binary_logloss: 0.486488\n","[300]\ttraining's binary_logloss: 0.425415\tvalid_1's binary_logloss: 0.48495\n","[400]\ttraining's binary_logloss: 0.422655\tvalid_1's binary_logloss: 0.483905\n","[500]\ttraining's binary_logloss: 0.420186\tvalid_1's binary_logloss: 0.48372\n","[600]\ttraining's binary_logloss: 0.417771\tvalid_1's binary_logloss: 0.482896\n","[700]\ttraining's binary_logloss: 0.41546\tvalid_1's binary_logloss: 0.483029\n","[800]\ttraining's binary_logloss: 0.413259\tvalid_1's binary_logloss: 0.482478\n","[900]\ttraining's binary_logloss: 0.411136\tvalid_1's binary_logloss: 0.481646\n","[1000]\ttraining's binary_logloss: 0.409085\tvalid_1's binary_logloss: 0.48094\n","[1100]\ttraining's binary_logloss: 0.407047\tvalid_1's binary_logloss: 0.480642\n","[1200]\ttraining's binary_logloss: 0.40518\tvalid_1's binary_logloss: 0.479938\n","[1300]\ttraining's binary_logloss: 0.403253\tvalid_1's binary_logloss: 0.479638\n","[1400]\ttraining's binary_logloss: 0.401397\tvalid_1's binary_logloss: 0.479604\n","[1500]\ttraining's binary_logloss: 0.399529\tvalid_1's binary_logloss: 0.478949\n","Early stopping, best iteration is:\n","[1465]\ttraining's binary_logloss: 0.400215\tvalid_1's binary_logloss: 0.478901\n","VALID3 AUC : 0.848867875513067 ACC : 0.7671930718288335\n","\n","[LightGBM] [Info] Number of positive: 1034940, number of negative: 551661\n","[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079198 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 17384\n","[LightGBM] [Info] Number of data points in the train set: 1586601, number of used features: 88\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652300 -> initscore=0.629165\n","[LightGBM] [Info] Start training from score 0.629165\n","Training until validation scores don't improve for 100 rounds\n","[100]\ttraining's binary_logloss: 0.4336\tvalid_1's binary_logloss: 0.476752\n","[200]\ttraining's binary_logloss: 0.428613\tvalid_1's binary_logloss: 0.47498\n","[300]\ttraining's binary_logloss: 0.42549\tvalid_1's binary_logloss: 0.475234\n","Early stopping, best iteration is:\n","[240]\ttraining's binary_logloss: 0.427282\tvalid_1's binary_logloss: 0.474261\n","VALID4 AUC : 0.8542978601751335 ACC : 0.771513353115727\n","\n","[LightGBM] [Info] Number of positive: 1035773, number of negative: 550409\n","[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154733 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 17393\n","[LightGBM] [Info] Number of data points in the train set: 1586182, number of used features: 88\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652998 -> initscore=0.632242\n","[LightGBM] [Info] Start training from score 0.632242\n","Training until validation scores don't improve for 100 rounds\n","[100]\ttraining's binary_logloss: 0.433836\tvalid_1's binary_logloss: 0.486356\n","[200]\ttraining's binary_logloss: 0.428694\tvalid_1's binary_logloss: 0.481384\n","[300]\ttraining's binary_logloss: 0.425649\tvalid_1's binary_logloss: 0.479165\n","[400]\ttraining's binary_logloss: 0.422935\tvalid_1's binary_logloss: 0.477605\n","[500]\ttraining's binary_logloss: 0.420402\tvalid_1's binary_logloss: 0.47708\n","[600]\ttraining's binary_logloss: 0.418081\tvalid_1's binary_logloss: 0.475834\n","[700]\ttraining's binary_logloss: 0.415767\tvalid_1's binary_logloss: 0.475294\n","[800]\ttraining's binary_logloss: 0.413551\tvalid_1's binary_logloss: 0.475345\n","Early stopping, best iteration is:\n","[728]\ttraining's binary_logloss: 0.41515\tvalid_1's binary_logloss: 0.475028\n","VALID5 AUC : 0.8520069449303758 ACC : 0.7648902821316614\n","\n"]}],"source":["model_list = []\n","auc_list = []\n","acc_list = []\n","\n","for cv_num in range(1,6):\n","    users_file_path = os.path.join(data_dir, f'cv{cv_num}_users.pickle')\n","    with open(users_file_path,'rb') as f:\n","        users = pickle.load(f)\n","    train_users = users['train_users']\n","\n","    # 유저별 분리\n","    train, valid = custom_train_test_split(train_df, user_ids=train_users)\n","\n","    # X, y 값 분리\n","    y_train = train['answerCode']\n","    train = train.drop(['answerCode'], axis=1)\n","\n","    y_valid = valid['answerCode']\n","    valid = valid.drop(['answerCode'], axis=1)\n","\n","    lgb_train = lgb.Dataset(train[FEATS], y_train)\n","    lgb_valid = lgb.Dataset(valid[FEATS], y_valid)\n","\n","    model = lgb.train(\n","        {'objective':'binary'},\n","        lgb_train,\n","        valid_sets=[lgb_train, lgb_valid],\n","        verbose_eval=100,\n","        num_boost_round=10000,\n","        early_stopping_rounds=100,\n","    )\n","\n","    preds = model.predict(valid[FEATS])\n","    acc = accuracy_score(y_valid, np.where(preds >= 0.5, 1, 0))\n","    auc = roc_auc_score(y_valid, preds)\n","\n","    print(f'VALID{cv_num} AUC : {auc} ACC : {acc}\\n')\n","    \n","    model_list.append(model)\n","    auc_list.append(auc)\n","    acc_list.append(acc)"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["AUC : 0.8482 ACC : 0.7677\n","\n"]}],"source":["print(f'AUC : {np.mean(auc_list):.4f} ACC : {np.mean(acc_list):.4f}\\n')"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["total_preds = np.zeros(len(test_df))\n","for model in model_list:\n","    total_preds += model.predict(test_df[FEATS]) / len(model_list)"]},{"cell_type":"code","execution_count":63,"metadata":{"ExecuteTime":{"end_time":"2021-05-24T09:49:29.694736Z","start_time":"2021-05-24T09:49:28.995Z"},"id":"f8PvohzwhfQ4"},"outputs":[{"name":"stdout","output_type":"stream","text":["writing prediction : output/estimate elapsedTime.csv\n"]}],"source":["# SAVE OUTPUT\n","output_dir = 'output/'\n","write_path = os.path.join(output_dir, \"estimate elapsedTime.csv\")\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","with open(write_path, 'w', encoding='utf8') as w:\n","    print(\"writing prediction : {}\".format(write_path))\n","    w.write(\"id,prediction\\n\")\n","    for id, p in enumerate(total_preds):\n","        w.write('{},{}\\n'.format(id,p))"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"(미션-2) LGBM Baseline.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":0}
